import streamlit as st
from streamlit_extras.dataframe_explorer import dataframe_explorer
import numpy as np
import pandas as pd
from transformers import pipeline
from PIL import  Image
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()
import plotly.express as px
import nltk 
  
def intro():
    import streamlit as st
    # Title of the application 
    st.write("# –î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å! üëã")
    st.sidebar.success("–í—ã–±—Ä–∞—Ç—å —Ä–∞–∑–¥–µ–ª")

    st.markdown(' # –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —á–∞—Ç–æ–≤ –¢–µ–ª–µ–≥—Ä–∞–º\n', )
    st.info("–ì—Ä—É–ø–ø–∞ 32: –°–º–∏—Ä–Ω–æ–≤–∞ –ê., –ö–æ–∂–µ–¥—É–± –ù., –ë–∞–≥–∞—É–¥–∏–Ω–æ–≤ –≠., –ü–µ—Ç—Ä–∞–∫–æ–≤ –í.")

    display = Image.open('images/display.jpg')
    display = np.array(display)
    st.image(display)
   

def mapping_demo():
    import streamlit as st
    from streamlit_extras.dataframe_explorer import dataframe_explorer
    import pandas as pd
    from transformers import pipeline
    import matplotlib.pyplot as plt 
    import seaborn as sns; sns.set()
    import plotly.express as px

    st.markdown(f"# {list(page_names_to_funcs.keys())[1]}")
    @st.cache 
    @st.experimental_memo
    def read_data(uploaded_file):
        return pd.read_csv(uploaded_file)
    datafile = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª csv", ["csv"])
   
    if datafile is None:
        st.info("""–ó–∞–≥—Ä—É–∑–∏—Ç–µ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö (.csv), —á—Ç–æ–±—ã –ø—Ä–∏—Å—Ç—É–ø–∏—Ç—å –∫ —Ä–∞–±–æ—Ç–µ.""")
        st.stop() 
    

    data = read_data(datafile).copy()
   
    model=pipeline("sentiment-analysis",   
                      "blanchefort/rubert-base-cased-sentiment")
                      
    result = st.button('–†–∞—Å–ø–æ–∑–Ω–∞—Ç—å')
    st.balloons()
    
    df_model = data.copy()
   
    if result:
        lst = []
        for i in df_model["text"]:
            lst.append(model(str(i))[0]["label"])
            df_model["Sentinent"]=pd.DataFrame(lst)
        st.write(df_model)
        st.balloons()

        st.subheader("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–æ–≤ —Å–æ–æ–±—â–µ–Ω–∏–π")

        chat_df = pd.DataFrame(df_model["Sentinent"].dropna().value_counts()).reset_index()
        chat_df = chat_df.sort_values(by="index")
        chat_df.columns = ["Sentinent", "Count"]
        fig = px.bar(
        chat_df,
        x="Sentinent",
        y="Count",
        title="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–æ–≤ —Å–æ–æ–±—â–µ–Ω–∏–π",
        color_discrete_sequence=["#9EE6CF"],)
        st.plotly_chart(fig, theme="streamlit", use_container_width=True)
        
        st.subheader("–°–∞–º—ã–µ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ user_id")
        df_P = df_model[df_model["Sentinent"]=="POSITIVE"]
        df_p =df_P.user_id.value_counts().sort_index()
        df_p.plot.bar(edgecolor='k', alpha=0.9, stacked = True, cmap="viridis")
# Create a word cloud function 
def wordcloud():
   # Standard Libraries
   import streamlit as st
   import os 
   import re 
   import string 
   import numpy as np
   from collections import Counter
   # Text Processing Library 
   from nltk.corpus import stopwords
   import pandas as pd
   import pymorphy2
   from nltk.tokenize import word_tokenize 
  
   # Data Visualisation 
   import matplotlib.pyplot as plt 
   from wordcloud import WordCloud 
   from PIL import Image
   
   st.markdown(f"# {list(page_names_to_funcs.keys())[2]}")
   st.header("Generate Word Cloud")
   st.subheader("Generate a word cloud from text containing the most popular words in the text.")

   # –ó–∞–ø—Ä–æ—Å–∏—Ç—å —Ç–µ–∫—Å—Ç –∏–ª–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
    
   # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫–æ–ª–æ–Ω–∫–∏ 'Title'
   text = ' '.join(data['Text'])
   # —Ä–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Ç–æ–∫–µ–Ω—ã
   # –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –ø–æ–ª—É—á–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é —Ç–∏–ø–∞ list —Å–æ —Å–ø–∏—Å–∫–æ–º —Ç–æ–∫–µ–Ω–æ–≤
   text = word_tokenize(text)
   # –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ª–µ–º–º–∞—Ç–∞–π–∑–µ—Ä MorphAnalyzer()
   lemmatizer = pymorphy2.MorphAnalyzer()
   # —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞, –Ω–∞ –≤—Ö–¥ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ 
   def lemmatize_text(tokens):
    # —Å–æ–∑–¥–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    text_new=''
    # –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤ —Ç–µ–∫—Å—Ç–µ
    for word in tokens:
        # —Å –ø–æ–º–æ—â—å—é –ª–µ–º–º–∞—Ç–∞–π–∑–µ—Ä–∞ –ø–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Ñ–æ—Ä–º—É
        word = lemmatizer.parse(word)
        # –¥–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—É—á–µ–Ω–Ω—É—é –ª–µ–º–º—É –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é —Å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º
        text_new = text_new + ' ' + word[0].normal_form
    # –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    return text_new
 
    # –≤—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
   text = lemmatize_text(text)

    # –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–±–ª–∞–∫–æ —Å–ª–æ–≤
   cloud = WordCloud(stopwords=stop_words).generate(text)
   plt.imshow(cloud)
   plt.axis('off')

page_names_to_funcs = {
    "–ì–ª–∞–≤–Ω–∞—è üëã": intro,
    "–ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–æ–≤ üî≠": mapping_demo,
    "create_wordcloud ":wordcloud
   }
name = st.sidebar.selectbox("–í—ã–±—Ä–∞—Ç—å —Ä–∞–∑–¥–µ–ª", page_names_to_funcs.keys())
page_names_to_funcs[name]()